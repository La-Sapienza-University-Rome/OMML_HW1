{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Sigma: 1\n",
      "N: 60\n",
      "Rho: 1e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000638\n",
      "         Iterations: 3773\n",
      "         Function evaluations: 3887\n",
      "         Gradient evaluations: 3887\n",
      "N\n",
      "60\n",
      "\n",
      "sigma\n",
      "1\n",
      "\n",
      "rho\n",
      "1e-05\n",
      "\n",
      "W\n",
      "(array([[ 3.07219476e-01,  1.45837132e-02, -1.09956235e-01,\n",
      "        -1.90521111e-02, -1.87480539e-02, -8.96341800e-04,\n",
      "         1.73494306e+00,  1.10533402e-02,  1.50414356e-02,\n",
      "        -6.84604360e-03,  6.42155034e-03,  1.95610802e-02,\n",
      "        -1.42203817e-02,  1.34401004e-02,  4.30476243e-03,\n",
      "        -8.26751600e-03,  9.09236884e-04, -1.48695727e-02,\n",
      "        -2.26386403e-02, -1.57917675e-02, -1.43545773e-02,\n",
      "        -6.64233872e-01, -1.42164981e-02, -1.06922596e-02,\n",
      "         9.94933478e-03,  1.14721434e-02, -1.19042444e+00,\n",
      "        -1.29155218e-02, -8.28473174e-01, -8.69712812e-01,\n",
      "        -1.63355170e-02,  5.50126715e-01, -8.35337060e-03,\n",
      "        -1.28145108e+00,  8.40221602e-03,  9.19293435e-03,\n",
      "         1.04313049e-02, -1.54887069e+00, -1.36799014e+00,\n",
      "         1.28073356e-02,  2.05244791e-02,  1.89223863e-02,\n",
      "        -1.48386071e+00, -3.16281260e-01, -1.25015044e+00,\n",
      "        -2.38670924e-02,  2.14813568e-02, -4.79783349e-02,\n",
      "         1.15226225e-02, -1.69380492e+00,  2.57067116e-01,\n",
      "        -1.28092004e-02, -1.13769608e-02,  3.76533677e-01,\n",
      "         5.65448201e-01,  1.30025510e-02,  1.63143870e-02,\n",
      "        -1.22301192e-02,  8.45626523e-01,  1.67825913e+00],\n",
      "       [-1.11260793e+00, -2.39587290e-02, -9.89368217e-01,\n",
      "         2.51433302e-02,  2.15728559e-02,  1.99585033e-02,\n",
      "        -1.14032662e-01, -3.17141384e-02, -2.26648166e-02,\n",
      "         3.46455847e-02, -4.30689847e-02, -2.36761130e-02,\n",
      "         2.74741657e-02, -2.16982344e-02, -1.63763773e-02,\n",
      "         1.24356150e-02, -2.40957395e-02,  2.67189193e-02,\n",
      "         3.43808033e-02,  2.31785189e-02,  1.45478878e-02,\n",
      "         5.53503690e-01,  2.55551230e-02,  3.25429779e-02,\n",
      "        -2.53659786e-02, -2.10480417e-02,  5.56293135e-01,\n",
      "         2.32172350e-02, -4.50325008e-01,  7.50288421e-01,\n",
      "        -1.25095043e-02,  5.50587386e-01,  2.13188722e-02,\n",
      "        -4.34996075e-01, -2.90606089e-02, -1.50622705e-02,\n",
      "        -2.14438860e-02,  1.23583304e-01,  2.92468684e-01,\n",
      "        -2.32767256e-02, -1.37032096e-02, -1.72171945e-02,\n",
      "        -1.95231382e-01, -1.34616760e+00, -5.86692613e-01,\n",
      "         1.28893996e-02, -2.27320017e-02,  1.07976437e+00,\n",
      "        -2.60902491e-02,  5.03641074e-02, -1.35677014e+00,\n",
      "         1.92674427e-02,  2.64918925e-02,  1.40485935e+00,\n",
      "        -8.33934219e-01, -1.71878782e-02,  1.68491357e-02,\n",
      "         3.12101400e-02,  9.11840866e-01,  1.55342695e-01]]),)\n",
      "\n",
      "b\n",
      "(array([ 0.5793459 ,  0.22887474,  1.69073545, -0.26927642, -0.14674476,\n",
      "       -0.08304906, -1.72066838,  0.19858339,  0.20573561, -0.25199894,\n",
      "        0.19294507,  0.1147612 , -0.38012016,  0.29509518,  0.04218249,\n",
      "       -0.07864851,  0.07314706, -0.22846906, -0.13563075, -0.31563641,\n",
      "       -0.08957638,  1.90929728, -0.3584462 , -0.15021572,  0.38936463,\n",
      "        0.39150599, -0.4531755 , -0.38762604,  1.02420769, -0.0068526 ,\n",
      "        0.0396399 ,  0.58351513, -0.39158217, -0.75429947,  0.34419903,\n",
      "        0.04944598,  0.39604415,  0.49799951, -2.17229749,  0.37597921,\n",
      "        0.10720183,  0.41160686,  2.20505275, -1.01601338,  0.57308179,\n",
      "       -0.17075386,  0.14214002,  2.13855619,  0.12579786, -1.76297625,\n",
      "       -0.77706196, -0.3182115 , -0.33985787, -0.56660457,  0.82328223,\n",
      "        0.09028828,  0.08480192, -0.1713362 , -0.35958676,  2.78540136]),)\n",
      "\n",
      "v\n",
      "[ 1.19418201 -0.22768526 -1.41960087  0.28283416  0.15478588  0.06148791\n",
      "  1.4760345  -0.20510035 -0.19616264  0.26672363 -0.18720329 -0.1278985\n",
      "  0.41336627 -0.3050745  -0.04176935  0.07579411 -0.04645768  0.23231732\n",
      "  0.14607471  0.33839467  0.0868322   1.32680462  0.38846365  0.14353983\n",
      " -0.41305664 -0.4103162   0.56861149  0.41718899  1.20377336  0.54010941\n",
      " -0.05089243  0.76423588  0.41649082  1.12895104 -0.37126931 -0.05920892\n",
      " -0.43197103 -1.00947931 -1.88645215 -0.40822264 -0.12410051 -0.434047\n",
      "  1.78218212 -1.51634942 -0.45519356  0.16611488 -0.15311072 -1.51144288\n",
      " -0.11753536  1.73186322 -1.11675823  0.34770818  0.35683073 -1.14108409\n",
      "  1.01900824 -0.11067711 -0.06563567  0.16630942 -0.66293452  2.29527765]\n",
      "\n",
      "Convergence?\n",
      "True\n",
      "\n",
      "Best Method?\n",
      "BFGS\n",
      "\n",
      "Objective Function Start\n",
      "18.299572495167745\n",
      "\n",
      "Objective Function End\n",
      "0.0006376839047433941\n",
      "\n",
      "Gradient Norm Start\n",
      "40.448798869787204\n",
      "\n",
      "Gradient Norm End\n",
      "3.0090739324267704e-06\n",
      "\n",
      "Computation time\n",
      "29.89\n",
      "\n",
      "Final Train Error\n",
      "4.7335406968071445e-05\n",
      "\n",
      "Final Test Error\n",
      "0.00029554827459905527\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "df = pd.read_csv('DATA.csv')\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.255, random_state=1939671)\n",
    "\n",
    "X = np.array(train[['x1', 'x2']])\n",
    "y = np.array(train['y'])\n",
    "\n",
    "X_test = np.array(test[['x1', 'x2']])\n",
    "y_test = np.array(test['y'])\n",
    "\n",
    "\n",
    "def tanh(s, sigma):\n",
    "    prod = 2 * sigma * s\n",
    "    return (np.exp(prod) - 1) / (np.exp(prod) + 1)\n",
    "\n",
    "\n",
    "def feedforward(X, W, b, v, sigma):\n",
    "    linear_layer = (np.dot(X, W) + b)\n",
    "    activation = tanh(linear_layer, sigma)\n",
    "    pred = np.dot(activation, v)\n",
    "\n",
    "    return pred\n",
    "\n",
    "def backpropagation(x0, funcArgs):\n",
    "    \n",
    "    X = funcArgs[0]\n",
    "    y = funcArgs[1]\n",
    "    sigma = funcArgs[2]\n",
    "    N = funcArgs[3]\n",
    "    rho = funcArgs[4]\n",
    "    P = len(y)\n",
    "    \n",
    "    W = x0[:int(X.shape[1] * N)].reshape((X.shape[1], N))\n",
    "    b = x0[int(X.shape[1] * N):int(X.shape[1] * N + N)]\n",
    "    v = x0[int(X.shape[1] * N + N):]\n",
    "\n",
    "    linear_layer = (np.dot(X, W) + b)\n",
    "    a_2 = tanh(linear_layer, sigma)\n",
    "    dJdf = (1 / P) * (np.dot(a_2, v) - y)\n",
    "    dtanh = 1 - tanh(linear_layer, sigma) ** 2\n",
    "\n",
    "    dW1_1 = np.tensordot(dJdf, np.transpose(v), axes=0)\n",
    "    dW1_2 = dW1_1 * dtanh\n",
    "\n",
    "    dv = np.dot(dJdf, a_2) + rho * v\n",
    "    db = np.sum(dW1_2, axis=0) + rho * b\n",
    "    dW = np.tensordot(np.transpose(X), dW1_2, axes=1) + rho * W\n",
    "\n",
    "    return np.concatenate((dW, db, dv), axis=None)\n",
    "\n",
    "def loss(x0, funcArgs, test=False):\n",
    "    X = funcArgs[0]\n",
    "    y = funcArgs[1]\n",
    "    sigma = funcArgs[2]\n",
    "    N = funcArgs[3]\n",
    "    rho = funcArgs[4]\n",
    "\n",
    "    W = x0[:int(X.shape[1] * N)].reshape((X.shape[1], N))\n",
    "    b = x0[int(X.shape[1] * N):int(X.shape[1] * N + N)]\n",
    "    v = x0[int(X.shape[1] * N + N):]\n",
    "\n",
    "    P = len(y)\n",
    "    norm = np.linalg.norm(x0)\n",
    "    pred = feedforward(X, W, b, v, sigma)\n",
    "    if test:\n",
    "        res = ((np.sum((pred - y) ** 2)) * P ** (-1)) * 0.5\n",
    "    else:\n",
    "        res = ((np.sum((pred - y) ** 2)) * P ** (-1) + rho * norm**2) * 0.5\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def loss_test(X, y, sigma, W, b, v):\n",
    "    P = len(y)\n",
    "    pred = feedforward(X, W, b, v, sigma)\n",
    "    res = ((np.sum((pred - y) ** 2)) * P ** (-1)) * 0.5\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def feedforwardplot(x1, x2, W, b, v, sigma):\n",
    "    X = np.array([x1, x2])\n",
    "    linear_layer = (np.dot(X, W) + b)\n",
    "    activation = tanh(linear_layer, sigma)\n",
    "    pred = np.dot(activation, v)\n",
    "\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train(X, y, sigma, N, rho, W, b, v, max_iter=1000,\n",
    "          tol=1e-5, method='CG', func=loss, disp=False):\n",
    "          \n",
    "    x0 = np.concatenate((W, b, v), axis=None)\n",
    "    funcArgs = [X, y, sigma, N, rho]\n",
    "    \n",
    "    res = minimize(func,\n",
    "                   x0,\n",
    "                   args=funcArgs, \n",
    "                   method=method, \n",
    "                   tol=tol,\n",
    "                   jac=backpropagation,\n",
    "                   options={'maxiter':max_iter, \n",
    "                            'disp': disp})  \n",
    "    \n",
    "    return res\n",
    "    \n",
    "\n",
    "def plotting(W, b, v, sigma):\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    # create the grid\n",
    "    x = np.linspace(-3, 3, 50)\n",
    "    y = np.linspace(-2, 2, 50)\n",
    "    X_plot, Y_plot = np.meshgrid(x, y)\n",
    "\n",
    "    Z = []\n",
    "    for x1 in x:\n",
    "        z = []\n",
    "        for x2 in y:\n",
    "            z.append(feedforwardplot(x1, x2, W, b, v, sigma))\n",
    "        Z.append(z)\n",
    "    Z = np.array(Z)\n",
    "\n",
    "    ax.plot_surface(X_plot, Y_plot, Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.set_title('F(x) learnt from MLP BackPropagation')\n",
    "    plt.show()\n",
    "\n",
    "sigma = 1\n",
    "N = 60\n",
    "rho = 1e-5\n",
    "method = 'BFGS'\n",
    "\n",
    "W = np.random.normal(size=(X.shape[1], N))\n",
    "b = np.random.normal(size=N)\n",
    "v = np.random.normal(size=N)\n",
    "\n",
    "x0 = np.concatenate((W, b, v), axis=None)\n",
    "funcArgs = [X, y, sigma, N, rho]\n",
    "\n",
    "print('===================')\n",
    "print('Sigma:', sigma)\n",
    "print('N:', N)\n",
    "print('Rho:', rho)\n",
    "\n",
    "loss_start = loss(x0, funcArgs)\n",
    "grad_norm_start = np.linalg.norm(backpropagation(x0, \n",
    "                            funcArgs=funcArgs))\n",
    "\n",
    "start = time.time()\n",
    "res = train(X, y, sigma=sigma, \n",
    "            N=N, rho=rho, \n",
    "            W=W, b=b, v=v,\n",
    "            max_iter=5000, tol=1e-6, \n",
    "            method=method, func=loss,\n",
    "            disp=True)\n",
    "stop = time.time()\n",
    "\n",
    "loss_end = res.fun\n",
    "\n",
    "W=res.x[:int(X.shape[1] * N)].reshape((X.shape[1], N)),\n",
    "b=res.x[int(X.shape[1] * N):int(X.shape[1] * N + N)],\n",
    "v=res.x[int(X.shape[1] * N + N):]\n",
    "\n",
    "funcArgs_test = [X_test, y_test, sigma, N, rho]\n",
    "\n",
    "loss(x0, funcArgs)\n",
    "\n",
    "val_loss = loss(res.x, funcArgs_test, test=True)\n",
    "\n",
    "grad_norm_end = np.linalg.norm(backpropagation(np.concatenate((W, b, v), axis=None), \n",
    "                            funcArgs=funcArgs))\n",
    "\n",
    "train_loss = loss(res.x, funcArgs, test=True)\n",
    "\n",
    "best_loss_start = loss_start\n",
    "best_loss_end = loss_end\n",
    "N_best = N\n",
    "sigma_best = sigma\n",
    "rho_best = rho\n",
    "convergence = res.success\n",
    "method_best = method\n",
    "\n",
    "print('N')\n",
    "print(N_best)\n",
    "print('')\n",
    "print('sigma')\n",
    "print(sigma_best)\n",
    "print('')\n",
    "print('rho')\n",
    "print(rho_best)\n",
    "print('')\n",
    "print('W')\n",
    "print(W)\n",
    "print('')\n",
    "print('b')\n",
    "print(b)\n",
    "print('')\n",
    "print('v')\n",
    "print(v)\n",
    "print('')\n",
    "print('Convergence?')\n",
    "print(convergence)\n",
    "print('')\n",
    "print('Best Method?')\n",
    "print(method_best)\n",
    "print('')\n",
    "print('Objective Function Start')\n",
    "print(best_loss_start)\n",
    "print('')\n",
    "print('Objective Function End')\n",
    "print(best_loss_end)\n",
    "print('')\n",
    "print('Gradient Norm Start')\n",
    "print(grad_norm_start)\n",
    "print('')\n",
    "print('Gradient Norm End')\n",
    "print(grad_norm_end)\n",
    "print('')\n",
    "print('Computation time')\n",
    "print(round(stop-start, 2))\n",
    "print('')\n",
    "print('Final Train Error')\n",
    "print(train_loss)\n",
    "print('')\n",
    "print('Final Test Error')\n",
    "print(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Sigma: 1\n",
      "N: 60\n",
      "Rho: 1e-05\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001298\n",
      "         Iterations: 1188\n",
      "         Function evaluations: 1208\n",
      "         Gradient evaluations: 1208\n",
      "N\n",
      "60\n",
      "\n",
      "sigma\n",
      "1\n",
      "\n",
      "rho\n",
      "1e-05\n",
      "\n",
      "Convergence?\n",
      "True\n",
      "\n",
      "Best Method?\n",
      "BFGS\n",
      "\n",
      "Objective Function Start\n",
      "2.77404188131989\n",
      "\n",
      "Objective Function End\n",
      "0.001298469027917437\n",
      "\n",
      "Gradient Norm Start\n",
      "2.8760927732415857\n",
      "\n",
      "Gradient Norm End\n",
      "2.968273834681125e-06\n",
      "\n",
      "Computation time\n",
      "11.04\n",
      "\n",
      "Final Train Error\n",
      "0.0002660363630278291\n",
      "\n",
      "Final Test Error\n",
      "0.0006291415721330511\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "df = pd.read_csv('DATA.csv')\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.255, random_state=1939671)\n",
    "\n",
    "X = np.array(train[['x1', 'x2']])\n",
    "y = np.array(train['y'])\n",
    "\n",
    "X_test = np.array(test[['x1', 'x2']])\n",
    "y_test = np.array(test['y'])\n",
    "\n",
    "\n",
    "def rbf(X, c, sigma):\n",
    "    \"\"\"\n",
    "    This function is only applied for a single observation\n",
    "    x belongs to R^2\n",
    "    c belongs to R^{2, 10}\n",
    "    return R^10, 186\n",
    "    \"\"\"\n",
    "    minus_matrix = []\n",
    "    for i in range(len(c.T)):\n",
    "        minus_matrix.append(X - c.T[i])\n",
    "    minus_matrix = np.array(minus_matrix)\n",
    "\n",
    "    return np.exp(-(np.linalg.norm(minus_matrix, axis=2)/sigma)**2)\n",
    "\n",
    "\n",
    "def feedforward(X, c, v, sigma):\n",
    "    \"\"\"\n",
    "    This function is only applied for a single observation\n",
    "    x belongs to R^2\n",
    "    c belongs to R^{2, 10}\n",
    "    v belongs to R^N\n",
    "    return float\n",
    "    \"\"\"\n",
    "    \n",
    "    pred = np.dot(rbf(X, c, sigma).T, v)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def backpropagation(x0, funcArgs):\n",
    "\n",
    "    X = funcArgs[0]\n",
    "    y = funcArgs[1]\n",
    "    sigma = funcArgs[2]\n",
    "    N = funcArgs[3]\n",
    "    rho = funcArgs[4]\n",
    "    P = len(y)\n",
    "    \n",
    "    c = x0[:int(X.shape[1]*N)].reshape((X.shape[1],N))\n",
    "    v = x0[int(X.shape[1]*N):]\n",
    "    \n",
    "    z_1 = rbf(X, c, sigma).T\n",
    "    dJdf = (1/P)*(np.dot(z_1, v) - y)\n",
    "\n",
    "    minus_matrix = []\n",
    "    for i in range(len(c.T)):\n",
    "        minus_matrix.append(X - c.T[i])\n",
    "    minus_matrix = np.array(minus_matrix)\n",
    "\n",
    "    dW1_1 = np.dot(dJdf.reshape((P, 1)), v.reshape((1,N)))\n",
    "    dzdc = ((2*z_1)/(sigma**2))*minus_matrix.T\n",
    "\n",
    "    dv = np.dot(dJdf, z_1) + rho*v\n",
    "    dc = np.sum(dzdc*dW1_1, axis=1) + rho*c\n",
    "\n",
    "    return np.concatenate((dc, dv), axis=None)\n",
    "\n",
    "\n",
    "def loss(x0, funcArgs, test=False):\n",
    "    \n",
    "    X = funcArgs[0]\n",
    "    y = funcArgs[1]\n",
    "    sigma = funcArgs[2]\n",
    "    N = funcArgs[3]\n",
    "    rho = funcArgs[4]\n",
    "    \n",
    "    c = x0[:int(X.shape[1]*N)].reshape((X.shape[1],N))\n",
    "    v = x0[int(X.shape[1]*N):]\n",
    "\n",
    "    P = len(y)\n",
    "    pred = feedforward(X, c, v, sigma)\n",
    "    norm = np.linalg.norm(x0)\n",
    "    if test:\n",
    "        res = ((np.sum((pred - y) ** 2)) * P ** (-1)) * 0.5\n",
    "    else:\n",
    "        res = ((np.sum((pred - y) ** 2)) * P ** (-1) + rho * norm ** 2) * 0.5\n",
    "    \n",
    "    return res\n",
    "\n",
    "\n",
    "def feedforwardplot(x_i_1, x_i_2, c, v, sigma):\n",
    "    x_i = np.array([x_i_1, x_i_2])\n",
    "    pred = np.dot(np.exp(-(np.linalg.norm((x_i - c.T), axis=1)/sigma)**2), v)\n",
    "    return pred\n",
    "\n",
    "\n",
    "def train(X, y, sigma, N, rho, c_init, \n",
    "          v_init, max_iter=1000, tol=1e-5, method='CG', func=loss, disp=False):\n",
    "    \n",
    "    x0 = np.concatenate((c_init, v_init), axis=None)\n",
    "    funcArgs = [X, y, sigma, N, rho]\n",
    "\n",
    "    res = minimize(func,\n",
    "                   x0,\n",
    "                   args=funcArgs, \n",
    "                   method=method, \n",
    "                   tol=tol,\n",
    "                   jac=backpropagation,\n",
    "                   options={'maxiter':max_iter, \n",
    "                            'disp': disp})    \n",
    "    \n",
    "    return res\n",
    "\n",
    "sigma = 1\n",
    "N = 60\n",
    "rho = 1e-5\n",
    "method = 'BFGS'\n",
    "\n",
    "c = np.random.normal(size=(X.shape[1], N))\n",
    "v = np.random.normal(size=N)\n",
    "\n",
    "x0 = np.concatenate((c, v), axis=None)\n",
    "funcArgs = [X, y, sigma, N, rho]\n",
    "\n",
    "print('===================')\n",
    "print('Sigma:', sigma)\n",
    "print('N:', N)\n",
    "print('Rho:', rho)\n",
    "\n",
    "loss_start = loss(x0, funcArgs)\n",
    "grad_norm_start = np.linalg.norm(backpropagation(x0, \n",
    "                                 funcArgs=funcArgs))\n",
    "\n",
    "start = time.time()\n",
    "res = train(X, y, sigma=sigma, \n",
    "            N=N, rho=rho, \n",
    "            c_init=c, v_init=v,\n",
    "            max_iter=5000, tol=1e-6, \n",
    "            method=method, func=loss,\n",
    "            disp=True)\n",
    "stop = time.time()\n",
    "\n",
    "loss_end = res.fun\n",
    "\n",
    "funcArgs_test = [X_test, y_test, sigma, N, rho]\n",
    "\n",
    "loss(x0, funcArgs)\n",
    "\n",
    "val_loss = loss(res.x, funcArgs_test, test=True)\n",
    "\n",
    "grad_norm_end = np.linalg.norm(backpropagation(res.x, \n",
    "                               funcArgs=funcArgs))\n",
    "\n",
    "train_loss = loss(res.x, funcArgs, test=True)\n",
    "\n",
    "best_loss_start = loss_start\n",
    "best_loss_end = loss_end\n",
    "N_best = N\n",
    "sigma_best = sigma\n",
    "rho_best = rho\n",
    "convergence = res.success\n",
    "method_best = method\n",
    "\n",
    "print('N')\n",
    "print(N_best)\n",
    "print('')\n",
    "print('sigma')\n",
    "print(sigma_best)\n",
    "print('')\n",
    "print('rho')\n",
    "print(rho_best)\n",
    "print('')\n",
    "print('Convergence?')\n",
    "print(convergence)\n",
    "print('')\n",
    "print('Best Method?')\n",
    "print(method_best)\n",
    "print('')\n",
    "print('Objective Function Start')\n",
    "print(best_loss_start)\n",
    "print('')\n",
    "print('Objective Function End')\n",
    "print(best_loss_end)\n",
    "print('')\n",
    "print('Gradient Norm Start')\n",
    "print(grad_norm_start)\n",
    "print('')\n",
    "print('Gradient Norm End')\n",
    "print(grad_norm_end)\n",
    "print('')\n",
    "print('Computation time')\n",
    "print(round(stop-start, 2))\n",
    "print('')\n",
    "print('Final Train Error')\n",
    "print(train_loss)\n",
    "print('')\n",
    "print('Final Test Error')\n",
    "print(val_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
